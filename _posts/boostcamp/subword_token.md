# subword tokenize

The main advantage of a subword tokenizer is that it interpolates between word-based and character-based tokenization. Common words get a slot in the vocabulary, but the tokenizer can fall back to word pieces and individual characters for unknown words.

https://www.tensorflow.org/text/guide/subwords_tokenizer

For tokens not appearing in the original vocabulary, it is designed that they should be replaced with a special token [UNK], which stands for unknown token.

However, converting all unseen tokens into [UNK] will take away a lot of information from the input data. Hence, BERT makes use of a WordPiece algorithm that breaks a word into several subwords, such that commonly seen subwords can also be represented by the model.

https://albertauyeung.github.io/2020/06/19/bert-tokenization.html


## huggingface tokenizer
* segment embedding
```
encoded_input = tokenizer("How old are you?", "I'm 6 years old")
print(encoded_input)

{'input_ids': [101, 1731, 1385, 1132, 1128, 136, 102, 146, 112, 182, 127, 1201, 1385, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}
```

https://huggingface.co/transformers/preprocessing.html

* ìŠ¤í˜ì´ìŠ¤, ë§ˆì¹¨í‘œ, ì–¸ì–´ì— ë”°ë¥¸ rule based tokenizer

ê·¸ëƒ¥ í”íˆ ì•„ëŠ” í† í¬ë‚˜ì´ì €. ì´ ë°©ë²•ì˜ ë¬¸ì œ: ê·¸ëƒ¥ ë‹¨ì–´ ë³„ë¡œ ìª¼ê°œê²Œ ë˜ë©´... ì˜ì–´ì˜ íŠ¹ì„±ìƒ ì ‘ë‘ì‚¬, ì ‘ë¯¸ì‚¬ ê°™ì€ ê²ƒë“¤ë§ˆë‹¤ ë‹¨ì–´ 1ê°œì”© ë³„ê°œë¡œ í† í°ì´ ëœë‹¤. ë‹¨ì–´ ìˆ˜ê°€ ë§ì•„ì§. BERTì˜ ê²½ìš° ì„ë² ë”©ê¹Œì§€ í•™ìŠµì— í¬í•¨ëœë‹¤. ì„ë² ë”© ìª½ì— íŒŒë¼ë¯¸í„°ê°€ ì—„ì²­ ë§ì•„ì§„ë‹¤. ì‹¤ì œ ëª¨ë¸ ë¶€ë¶„ë³´ë‹¤ ì„ë² ë”© ìª½ì— íŒŒë¼ë¯¸í„°ê°€ ë” ë§ì•„ì§€ê¸°ë„ í•œë‹¤. íŒŒë¼ë¯¸í„°ê°€ ë§ì•„ì§€ë©´ ë©”ëª¨ë¦¬ë„ ë§ì´ ë¨¹ê³  ì‹œê°„ ë³µì¡ë„ë„ ì»¤ì§„ë‹¤. ê·¸ ë©”ëª¨ë¦¬ ì¤„ì—¬ì„œ ë¸”ëŸ­ì„ ë” ìŒ“ì„ ìˆ˜ëŠ” ì—†ì„ê¹Œ?

* character ê¸°ë°˜ tokenizer
ë‹¨ì–´ ë³„ë¡œ í† í°ì„ ìë¥´ë©´ ì˜ë¯¸ë¥¼ ë„ˆë¬´ ë§ì´ ìƒì–´ë²„ë¦°ë‹¤. ê·¸ë˜ì„œ í•™ìŠµì´ í˜ë“¤ì–´ì§„ë‹¤. todayì˜ tì™€ ê·¸ëƒ¥ tëŠ” ì˜ë¯¸ ì°¨ì´ê°€ ë„ˆë¬´ ë§ì´ ë‚œë‹¤.

* subword
character ê¸°ë°˜ì€ ë„ˆë¬´ ì˜ê²Œ ìª¼ê°œì„œ í•™ìŠµì´ ì–´ë µê³ , space ê¸°ë°˜ì€ ë„ˆë¬´ ì»¤ì„œ ë˜ ë©”ëª¨ë¦¬ì™€ í•™ìŠµ ì‹œê°„ì´ ë„ˆë¬´ í¬ë‹¤. 

ì¤‘ê°„ì€ ì—†ì„ê¹Œ? í•´ì„œ ë‚˜ì˜¨ ê²ƒì´ subword í† í°ì´ë‹¤. 

ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ê³¼ ê°€ë” ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì„ êµ¬ë¶„í•œë‹¤. ìì£¼ ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì€ ê·¸ ìì²´ë¡œ ì‚¬ì „ì— ì €ì¥í•˜ê³ , ê°€ë” ë‚˜ì˜¤ëŠ” ë‹¨ì–´ë“¤ì€ ì–´ë–»ê²Œë“  ìª¼ê° ë‹¤. 

i.e.
```
from transformers import BertTokenizer
tokenizer = BertTokenizer.from_pretrained("bert-base-uncased")
tokenizer.tokenize("I have a new GPU!")

["i", "have", "a", "new", "gp", "##u", "!"]
```

ìì£¼ ì•ˆë‚˜ì˜¤ëŠ”(í˜¹ì€ í† í¬ë‚˜ì´ì €ê°€ ì²˜ìŒë³´ëŠ”) GPUê°™ì€ ë‹¨ì–´ëŠ” Unitì˜ Uì™€ GPì„ ì˜ë¼ì„œ ì €ì¥í•œë‹¤. 
```
from transformers import XLNetTokenizer
tokenizer = XLNetTokenizer.from_pretrained("xlnet-base-cased")
tokenizer.tokenize("Don't you love ğŸ¤— Transformers? We sure do.")

["â–Don", "'", "t", "â–you", "â–love", "â–", "ğŸ¤—", "â–", "Transform", "ers", "?", "â–We", "â–sure", "â–do", "."]

```

ì´ ê²½ìš°ì—ë„ ë§ˆì°¬ê°€ì§€. ìì£¼ ì•ˆë‚˜ì˜¤ëŠ” Transformerë¼ëŠ” ë‹¨ì–´ë¥¼ Transformê³¼ erìœ¼ë¡œ êµ¬ë¶„í–ˆë‹¤. erë„ e ì™€ rì´ ì•„ë‹ˆë¼ erìœ¼ë¡œ ë‚˜ëˆ ì§„ ê²ƒë„ ì–´ëŠì •ë„ ë“±ì¥í•´ì„œ ì´ë ‡ê²Œ ëœ ê²ƒì´ë‹¤.

* ì›ë¦¬: ë„ëŒ€ì²´ ì–´ë–¤ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ?
Byte-Pair Encoding, WordPiece ë“±ì˜ ì•Œê³ ë¦¬ì¦˜ì´ ìˆìŒ. 

ëŒ€ë¶€ë¶„ ë³¸ê²©ì ì¸ í† í¬ë‚˜ì´ì§• ì „ì— ê·¸ëƒ¥ ë‹¨ì–´ ë³„ë¡œ ìª¼ê° ë‹¤. ê·¸ë¦¬ê³  ì¹´ìš´ë“œë¥¼ í•´ë‘”ë‹¤. ê·¸ë¦¬ê³  ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ...

## Byte-Pair Encoding
1. ì¹´ìš´ë“œ ë˜ì–´ ìˆëŠ” ë‹¨ì–´ë“¤ì´ ì¤€ë¹„ë˜ì—ˆë‹¤
2. ê° ë‹¨ì–´ë“¤ì„ char ë‹¨ìœ„ë¡œ ìª¼ê° ë‹¤
3. charê³¼ charì´ ê°€ì¥ í”í•˜ê²Œ ë¶™ëŠ” ê²½ìš°ë¥¼ ì°¾ì•„ì„œ í•©ì³ì„œ ë‘ê¸€ìë¡œ ë§Œë“ ë‹¤.
4. ê³„ì† ë°˜ë³µí•´ì„œ í•˜ì´í¼ íŒŒë¼ë¯¸í„°ë¡œ ì£¼ì–´ì§€ëŠ” ì‚¬ì „ ìˆ˜ê°€ ë ë•Œê¹Œì§€ ë°˜ë³µ!

GPT, RoBERTaì—ì„œ ì‚¬ìš©

Byte-level BPEëŠ” ë°”ì´íŠ¸ ë‹¨ìœ„ë¡œ í•œë‹¤ê³  í•œë‹¤... GPT2ì—ì„œ ì‚¬ìš©

## WordPiece
Byte Pair Encodingê³¼ ë¹„ìŠ·í•˜ë‹¤. ì°¨ì´ì ì€... ë¹ˆë„ ìˆ˜ë¡œ charê³¼ charì„ í•©ì¹˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼, trainingì´ ê°€ì¥ ìµœì í™”ë˜ëŠ” char ì¡°í•©ì„ ì°¾ëŠ”ë‹¤. `maximizing the likelihood of the training data`ì´ë‹¤. ê·¸ëŸ¬ë©´ ëª¨ë¸ ì „ì²´ í•™ìŠµ ê³¼ì •ì—ì„œ ì´ê²ƒë„ í•™ìŠµë˜ë‚˜?

BERT, DistilBERT, Electraì—ì„œ ì‚¬ìš©.

* ì´ê²ƒ ë§ê³ ë„ Unigramì´ ìˆë‹¤ê³  í•¨.




https://huggingface.co/transformers/tokenizer_summary.html
